{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cddfe265-313e-4e4e-813e-ee9094daa261",
   "metadata": {},
   "source": [
    "In this notebook I will show you how you can create an LLM based chatbot with Sentiment Analysis using OpenVINO.\n",
    "\n",
    "First we are going to import [OpenVINO](https://docs.openvino.ai/2023.1/api/ie_python_api/api.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6c45d6-969a-419f-a142-9b72ad519c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino as ov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ddddd2-8076-43fe-9f81-585fd4b2b28d",
   "metadata": {},
   "source": [
    "For this project we will be using pre-trained models from huggingface. There you can find models for a variety of tasks, including the ones we are interested in.\n",
    "\n",
    "In particular we are going to use [RedPajama-INCITE-Chat-3B-v1](https://huggingface.co/togethercomputer/RedPajama-INCITE-Chat-3B-v1) for the chat component, and [distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) for the sentiment analysis. You can visit those links to read more about the models.\n",
    "\n",
    "One of the simplest ways to work with huggingface models and OpenVINO is to use the [optimum](https://huggingface.co/docs/optimum/intel/index) library, which automates a lot of things for you.\n",
    "\n",
    "Depending on the task at hand, you will need to include different helpers from the optimum library. For text generation (chat), we will be using OVModelForCausalLM, and for Sentiment Analysis we will be using OVModelForSequenceClassification. You can see these defined here: https://huggingface.co/docs/optimum/intel/inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d426f785-eaa1-4503-ac37-73be0216e92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.intel.openvino import OVModelForCausalLM, OVModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573e940a-0c5e-4835-909c-f99a0a3866c6",
   "metadata": {},
   "source": [
    "Once you find your models from huggingface, make sure to copy the model names as you can see from the top of the website in the previous links. These are the ones we will be using:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795121e0-4ebe-453a-8514-8ada34263cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_chat = \"togethercomputer/RedPajama-INCITE-Chat-3B-v1\"\n",
    "model_name_sentiment = \"distilbert-base-uncased-finetuned-sst-2-english\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c282735-4c32-4e59-81bd-cb6190a4b59b",
   "metadata": {},
   "source": [
    "We can now simply use the helpers for each one of the models. Under the hood these functions will download and convert the models so that they can be used directly from OpenVINO.\n",
    "\n",
    "For each one you only need to provide either the model name as shown in the huggingface website, or a folder path that contains local model files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3bcc6e-7349-4d26-ab41-d46bfb1b0708",
   "metadata": {},
   "outputs": [],
   "source": [
    "ov_model_chat = OVModelForCausalLM.from_pretrained(model_name_chat, export=True, compile=False)\n",
    "ov_model_sentiment = OVModelForSequenceClassification.from_pretrained(model_name_sentiment, export=True, compile=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed898e7-29bc-40f1-928b-23e19100f157",
   "metadata": {},
   "source": [
    "You can explore the configuration details of each model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2e598a-3f22-4791-868d-d19f5548bb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Chat model:\", ov_model_chat.config)\n",
    "print(\"Sentiment model:\",ov_model_sentiment.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a4b0fd-517b-4480-896c-33a86e75fdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig #Helpers that make thing simpler for us.\n",
    "\n",
    "ov_model_chat_tok = AutoTokenizer.from_pretrained(model_name_chat, trust_remote_code=False)\n",
    "ov_model_sentiment_tok = AutoTokenizer.from_pretrained(model_name_sentiment, trust_remote_code=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e15a49-c1ed-4ee2-8213-c08ac789ce78",
   "metadata": {},
   "source": [
    "These models can be run offline and they were converted to the OpenVINO intermediate format, so let's make sure we save them in a convenient location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b1357f-eda1-4966-a5ba-c0e6a35cd712",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path_model_chat = Path(\"model_chat\")\n",
    "path_model_sentiment = Path(\"model_sentiment\")\n",
    "\n",
    "ov_model_chat.save_pretrained(path_model_chat)\n",
    "ov_model_chat_tok.save_pretrained(path_model_chat)\n",
    "\n",
    "ov_model_sentiment.save_pretrained(path_model_sentiment)\n",
    "ov_model_sentiment_tok.save_pretrained(path_model_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bed618-3676-498f-883b-effcd515a41d",
   "metadata": {},
   "source": [
    "You can now have a look at those two folders, each one will have three files representing the model (openvino_model.xml, openvino_model.bin, and config.json), and extra files for the AutoTokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ec159f-d6f3-4c7d-9a12-593d10c5b801",
   "metadata": {},
   "source": [
    "We can now use these models. Let's start with the LLM. \n",
    "\n",
    "Note that we are now supplying the folder path of the model instead of the name. This means that we are now directly using the OpenVINO optimised model files we saved in the previous step.\n",
    "\n",
    "After loading the model and the AutoTokenizer, we need to tokenize the input which the AutoTokenizer helper makes it simple to do. Then we can generate text by simply calling the generate method of the OpenVINO chat model and decoding it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaabafa-2fb0-41a4-bbb3-6dbf5a54dce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ov_config = {'PERFORMANCE_HINT': 'LATENCY', 'NUM_STREAMS': '1', \"CACHE_DIR\": \"\"}\n",
    "ov_model_chat_tok = AutoTokenizer.from_pretrained(path_model_chat)\n",
    "ov_model_chat = OVModelForCausalLM.from_pretrained(path_model_chat, device=\"CPU\", ov_config=ov_config, config=AutoConfig.from_pretrained(path_model_chat))\n",
    "\n",
    "tokenizer_kwargs = {}\n",
    "test_string = \"2 + 2 =\"\n",
    "input_tokens = ov_model_chat_tok(test_string, return_tensors=\"pt\", **tokenizer_kwargs)\n",
    "answer = ov_model_chat.generate(**input_tokens, max_new_tokens=2)\n",
    "print(ov_model_chat_tok.batch_decode(answer)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf567000-4a93-4890-8e68-dc3beff4b19a",
   "metadata": {},
   "source": [
    "Now it is time to test the sentiment analysis model.\n",
    "\n",
    "By using the helper functions we can do this very easily.\n",
    "\n",
    "We will be using the pipeline to get text classification directly.\n",
    "\n",
    "The output returns two values, either POSITIVE or NEGATIVE, and a score. I simply convert those into a single integer that contains the same information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb22f69d-d0e9-402b-9fac-791adc171199",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ov_sentiment_config = {'PERFORMANCE_HINT': 'LATENCY', 'NUM_STREAMS': '1', \"CACHE_DIR\": \"\"}\n",
    "ov_model_sentiment_tok = AutoTokenizer.from_pretrained(path_model_sentiment, trust_remote_code=False)\n",
    "ov_model_sentiment = OVModelForSequenceClassification.from_pretrained(path_model_sentiment, device=\"CPU\", ov_sentiment_config=ov_sentiment_config, config=AutoConfig.from_pretrained(path_model_sentiment))\n",
    "tokenizer_sentiment_kwargs = {}\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=ov_model_sentiment, tokenizer=ov_model_sentiment_tok)\n",
    "\n",
    "def get_sentiment(text):    \n",
    "    outputs = pipe(text)\n",
    "    sentiment = outputs[0][\"label\"]\n",
    "    sentiment_score = outputs[0][\"score\"]\n",
    "    if(sentiment == \"NEGATIVE\"):\n",
    "        sentiment_score *= -1\n",
    "    return sentiment_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcb57cc-8f08-4002-becf-fe4c6e1af98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"hello, this is great\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f59514-9ed4-46a6-8215-f6d5e8fb948f",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_sentiment(\"I don't like this at all!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2878e469-9d00-44ee-a96e-5f183a74c9ae",
   "metadata": {},
   "source": [
    "Now we are ready to create a chat with sentiment analysis in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ff1c07-9e3b-4ff4-bbeb-0bea2a4caef4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from threading import Event, Thread\n",
    "from uuid import uuid4\n",
    "from typing import Optional, Union, Dict, Tuple, List\n",
    "\n",
    "\n",
    "import gradio as gr\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    "    TextIteratorStreamer,\n",
    ")\n",
    "\n",
    "#This is the expected format for the model\n",
    "history_template = \"\\n<human>:{user}\\n<bot>:{assistant}\"\n",
    "current_message_template = \"\\n<human>:{user}\\n<bot>:{assistant}\"\n",
    "start_message = \"\"\n",
    "stop_tokens = [29, 0]\n",
    "tokenizer_kwargs = {}\n",
    "\n",
    "def red_pijama_partial_text_processor(partial_text, new_text):\n",
    "    if new_text == '<':\n",
    "        return partial_text\n",
    "    \n",
    "    partial_text += new_text\n",
    "    return partial_text.split('<bot>:')[-1]\n",
    "\n",
    "max_new_tokens = 256\n",
    "\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __init__(self, token_ids):\n",
    "        self.token_ids = token_ids\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_id in self.token_ids:\n",
    "            if input_ids[0][-1] == stop_id:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "if stop_tokens is not None:\n",
    "    if isinstance(stop_tokens[0], str):\n",
    "        stop_tokens = tok.convert_tokens_to_ids(stop_tokens)\n",
    "        \n",
    "    stop_tokens = [StopOnTokens(stop_tokens)]\n",
    "\n",
    "def default_partial_text_processor(partial_text:str, new_text:str):\n",
    "    \"\"\"\n",
    "    helper for updating partially generated answer, used by de\n",
    "    \n",
    "    Params:\n",
    "      partial_text: text buffer for storing previosly generated text\n",
    "      new_text: text update for the current step\n",
    "    Returns:\n",
    "      updated text string\n",
    "    \n",
    "    \"\"\"\n",
    "    partial_text += new_text\n",
    "    return partial_text\n",
    "\n",
    "text_processor = red_pijama_partial_text_processor\n",
    "\n",
    "\n",
    "def convert_history_to_text(history:List[Tuple[str, str]]):\n",
    "    \"\"\"\n",
    "    function for conversion history stored as list pairs of user and assistant messages to string according to model expected conversation template\n",
    "    Params:\n",
    "      history: dialogue history\n",
    "    Returns:\n",
    "      history in text format\n",
    "    \"\"\"\n",
    "    text = start_message + \"\".join(\n",
    "        [\n",
    "            \"\".join(\n",
    "                [\n",
    "                    history_template.format(user=item[0], assistant=item[1])\n",
    "                ]\n",
    "            )\n",
    "            for item in history[:-1]\n",
    "        ]\n",
    "    )\n",
    "    text += \"\".join(\n",
    "        [\n",
    "            \"\".join(\n",
    "                [\n",
    "                    current_message_template.format(user=history[-1][0], assistant=history[-1][1])\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def user(message, history):\n",
    "    \"\"\"\n",
    "    callback function for updating user messages in interface on submit button click\n",
    "    \n",
    "    Params:\n",
    "      message: current message\n",
    "      history: conversation history\n",
    "    Returns:\n",
    "      None\n",
    "    \"\"\"\n",
    "\n",
    "    # Append the user's message to the conversation history\n",
    "    return \"\", history + [[message, \"\"]]\n",
    "\n",
    "\n",
    "def bot(history, temperature, top_p, top_k, repetition_penalty, conversation_id):\n",
    "    \"\"\"\n",
    "    callback function for running chatbot on submit button click\n",
    "    \n",
    "    Params:\n",
    "      history: conversation history\n",
    "      temperature:  parameter for control the level of creativity in AI-generated text. \n",
    "                    By adjusting the `temperature`, you can influence the AI model's probability distribution, making the text more focused or diverse.\n",
    "      top_p: parameter for control the range of tokens considered by the AI model based on their cumulative probability.\n",
    "      top_k: parameter for control the range of tokens considered by the AI model based on their cumulative probability, selecting number of tokens with highest probability.\n",
    "      repetition_penalty: parameter for penalizing tokens based on how frequently they occur in the text.\n",
    "      conversation_id: unique conversation identifier.\n",
    "    \n",
    "    \"\"\"\n",
    "    user_message = history[-1][0]\n",
    "    user_sentiment = get_sentiment(user_message)\n",
    "\n",
    "    # Construct the input message string for the model by concatenating the current system message and conversation history\n",
    "    messages = convert_history_to_text(history)\n",
    "\n",
    "    \n",
    "    # Tokenize the messages string\n",
    "    input_ids = ov_model_chat_tok(messages, return_tensors=\"pt\", **tokenizer_kwargs).input_ids\n",
    "    if input_ids.shape[1] > 2000:\n",
    "        history = [history[-1]]\n",
    "        messages = convert_history_to_text(history)\n",
    "        input_ids = ov_model_chat_tok(messages, return_tensors=\"pt\", **tokenizer_kwargs).input_ids\n",
    "    streamer = TextIteratorStreamer(ov_model_chat_tok, timeout=30.0, skip_prompt=True, skip_special_tokens=True)\n",
    "    generate_kwargs = dict(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        do_sample=temperature > 0.0,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        streamer=streamer,\n",
    "    )\n",
    "    if stop_tokens is not None:\n",
    "        generate_kwargs[\"stopping_criteria\"] = StoppingCriteriaList(stop_tokens)\n",
    "\n",
    "    stream_complete = Event()\n",
    "\n",
    "    def generate_and_signal_complete():\n",
    "        \"\"\"\n",
    "        generation function for single thread\n",
    "        \"\"\"\n",
    "        global start_time\n",
    "        ov_model_chat.generate(**generate_kwargs)\n",
    "        stream_complete.set()\n",
    "\n",
    "    t1 = Thread(target=generate_and_signal_complete)\n",
    "    t1.start()\n",
    "\n",
    "    # Initialize an empty string to store the generated text\n",
    "    partial_text = \"\"\n",
    "    for new_text in streamer:\n",
    "        partial_text = text_processor(partial_text, new_text)\n",
    "        history[-1][1] = partial_text\n",
    "        history[-1][1] = \"**USER_SENTIMENT: \" + str(user_sentiment) + \"**\\n\" + history[-1][1]\n",
    "        yield history\n",
    "    \n",
    "\n",
    "def get_uuid():\n",
    "    \"\"\"\n",
    "    universal unique identifier for thread\n",
    "    \"\"\"\n",
    "    return str(uuid4())\n",
    "\n",
    "\n",
    "with gr.Blocks(\n",
    "    theme=gr.themes.Soft(),\n",
    "    css=\".disclaimer {font-variant-caps: all-small-caps;}\",\n",
    ") as demo:\n",
    "    conversation_id = gr.State(get_uuid)\n",
    "    gr.Markdown(\n",
    "        f\"\"\"<h1><center>OpenVINO Chatbot with Sentiment Analysis</center></h1>\"\"\"\n",
    "    )\n",
    "    chatbot = gr.Chatbot(height=800)\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            msg = gr.Textbox(\n",
    "                label=\"Chat Message Box\",\n",
    "                placeholder=\"Chat Message Box\",\n",
    "                show_label=False,\n",
    "                container=False\n",
    "            )\n",
    "        with gr.Column():\n",
    "            with gr.Row():\n",
    "                submit = gr.Button(\"Submit\")\n",
    "                stop = gr.Button(\"Stop\")\n",
    "                clear = gr.Button(\"Clear\")\n",
    "    with gr.Row():\n",
    "        with gr.Accordion(\"Advanced Options:\", open=False):\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    with gr.Row():\n",
    "                        temperature = gr.Slider(\n",
    "                            label=\"Temperature\",\n",
    "                            value=0.1,\n",
    "                            minimum=0.0,\n",
    "                            maximum=1.0,\n",
    "                            step=0.1,\n",
    "                            interactive=True,\n",
    "                            info=\"Higher values produce more diverse outputs\",\n",
    "                        )\n",
    "                with gr.Column():\n",
    "                    with gr.Row():\n",
    "                        top_p = gr.Slider(\n",
    "                            label=\"Top-p (nucleus sampling)\",\n",
    "                            value=1.0,\n",
    "                            minimum=0.0,\n",
    "                            maximum=1,\n",
    "                            step=0.01,\n",
    "                            interactive=True,\n",
    "                            info=(\n",
    "                                \"Sample from the smallest possible set of tokens whose cumulative probability \"\n",
    "                                \"exceeds top_p. Set to 1 to disable and sample from all tokens.\"\n",
    "                            ),\n",
    "                        )\n",
    "                with gr.Column():\n",
    "                    with gr.Row():\n",
    "                        top_k = gr.Slider(\n",
    "                            label=\"Top-k\",\n",
    "                            value=50,\n",
    "                            minimum=0.0,\n",
    "                            maximum=200,\n",
    "                            step=1,\n",
    "                            interactive=True,\n",
    "                            info=\"Sample from a shortlist of top-k tokens — 0 to disable and sample from all tokens.\",\n",
    "                        )\n",
    "                with gr.Column():\n",
    "                    with gr.Row():\n",
    "                        repetition_penalty = gr.Slider(\n",
    "                            label=\"Repetition Penalty\",\n",
    "                            value=1.1,\n",
    "                            minimum=1.0,\n",
    "                            maximum=2.0,\n",
    "                            step=0.1,\n",
    "                            interactive=True,\n",
    "                            info=\"Penalize repetition — 1.0 to disable.\",\n",
    "                        )\n",
    "    submit_event = msg.submit(\n",
    "        fn=user,\n",
    "        inputs=[msg, chatbot],\n",
    "        outputs=[msg, chatbot],\n",
    "        queue=False,\n",
    "    ).then(\n",
    "        fn=bot,\n",
    "        inputs=[\n",
    "            chatbot,\n",
    "            temperature,\n",
    "            top_p,\n",
    "            top_k,\n",
    "            repetition_penalty,\n",
    "            conversation_id,            \n",
    "        ],\n",
    "        outputs=chatbot,\n",
    "        queue=True,\n",
    "    )\n",
    "    submit_click_event = submit.click(\n",
    "        fn=user,\n",
    "        inputs=[msg, chatbot],\n",
    "        outputs=[msg, chatbot],\n",
    "        queue=False,\n",
    "    ).then(\n",
    "        fn=bot,\n",
    "        inputs=[\n",
    "            chatbot,\n",
    "            temperature,\n",
    "            top_p,\n",
    "            top_k,\n",
    "            repetition_penalty,\n",
    "            conversation_id,\n",
    "        ],\n",
    "        outputs=chatbot,\n",
    "        queue=True,\n",
    "    )\n",
    "    stop.click(\n",
    "        fn=None,\n",
    "        inputs=None,\n",
    "        outputs=None,\n",
    "        cancels=[submit_event, submit_click_event],\n",
    "        queue=False,\n",
    "    )\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "demo.queue(max_size=2)\n",
    "# if you are launching remotely, specify server_name and server_port\n",
    "#  demo.launch(server_name='your server name', server_port='server port in int')\n",
    "# if you have any issue to launch on your platform, you can pass share=True to launch method:\n",
    "# demo.launch(share=True)\n",
    "# it creates a publicly shareable link for the interface. Read more in the docs: https://gradio.app/docs/\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f18580-d545-409b-902f-e9edfa1b980b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343da43f-fa85-4e1e-a178-db5bbb3158fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# please run this cell for stopping gradio interface\n",
    "#demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58c36df-76de-4aef-86ca-b234ec9240f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
